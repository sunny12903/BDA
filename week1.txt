						HADOOP ESSENTIALS
This Manual provides thebasic Categorization of Cloud basics towork on scenario based 
learning on different Ecosystems.
1 SOFTWARE STUDY -HADOOP AND ECOSYSTEM OF HADOOP:
	1.1 OBJECTIVE
		How to Manage the Hadoop ecosystem
	1.2 PROCEDURE
		Source : Edureka(blog/hadoop-ecosystem) https://www.edureka.co/blog/hadoopecosystem
Hadoop Ecosystem is neither a programming language nor a service, it is a platform or 
framework which solves big data problems. You can consider it as a suite which encompasses a 
number of services (ingesting, storing, analyzing and maintaining) inside it. Let us discuss and 
get a brief idea about how the services work individually and in collaboration.
Below are the Hadoop components,that together form a Hadoop ecosystem, I will be covering 
each of them in this blog:
• HDFS -> Hadoop Distributed File System
• YARN -> Yet Another Resource Negotiator
• MapReduce -> Data processing using programming
• Spark -> In-memory Data Processing
• PIG, HIVE-> Data Processing Services using Query (SQL-like)
• HBase -> NoSQL Database
• Mahout, Spark MLlib -> Machine Learning
• Apache Drill -> SQL on Hadoop
• Zookeeper -> ManagingCluster
• Oozie -> Job Scheduling
• Flume, Sqoop -> Data Ingesting Services
• Solr & Lucene -> Searching & Indexing
• Ambari -> Provision, Monitor and Maintain cluster




			HDFS
• HadoopDistributed FileSystem is the core component or you can say,thebackbone of 
Hadoop Ecosystem.
• HDFS is the one, which makes it possible to store different types of large data sets (i.e. 
structured, unstructured and semi structured data).
• HDFScreates a levelofabstractionovertheresources,fromwherewecanseethewhole 
HDFS as a single unit.
• It helps us in storing our data across various nodes and maintaining the log file about 
the stored data (metadata).
• HDFS has two core components, i.e. NameNode and DataNode.
1. TheNameNode is the main node and it doesn’t store the actual data.It contains 
metadata, just like a log file or you can say as a table of content. Therefore, it 
requires less storage and high computational resources.
2. On the other hand, all your data is stored on the DataNodes and hence it 
requires more storage resources. These DataNodes are commodity hardware 
(like your laptops and desktops) in the distributed environment. That’s the 
reason, why Hadoop solutions are very cost effective.
3. You always communicate to the NameNode while writing the data. Then, it 
internally sends a request to the client to store and replicate data on various 
DataNodes.



YARN

Consider YARN as the brain of your Hadoop Ecosystem. It performs all your processing 
activities by allocating resources and scheduling tasks.
• It has two major components, i.e. ResourceManager and NodeManager.
1. ResourceManager is again a main node in the processing department.
2. It receives the processing requests, and then passes the parts of requests to 
corresponding NodeManagers accordingly, where the actual processing takes 
place.
3. NodeManagers are installed on every DataNode. It is responsible for execution 
of task on every single DataNode.
•



MAPREDUCE

It is the core component of processing in a Hadoop Ecosystem as it provides the logic of 
processing. In other words, MapReduce is a software framework which helps in writing 
applications that processes large data sets using distributed and parallel algorithms inside 
Hadoop environment.
• In a MapReduce program, Map() and Reduce() are two functions.
1. The Map function performs actions like filtering, grouping and sorting.
2. WhileReduce function aggregates and summarizes the result produced by map 
function.
3. The result generated by the Map function is a key value pair(K, V) which acts as 
the input for Reduce function.




APACHE PIG

• PIG has two parts: Pig Latin, the language and the pig runtime, for the execution 
environment. You can better understand it as Java and JVM.
• It supports pig latin language, which has SQL like command structure.
As everyone does not belong from a programming background. So, Apache PIG relieves 
them. You might be curious to know how?
Well, I will tell you an interesting fact:
10 line of pig latin = approx. 200 lines of Map-Reduce Java code
• It gives you a platform for building data flow for ETL (Extract, Transform and Load), 
processing and analyzing huge data sets.


APACHE HIVE

• Facebook created HIVE for people who are fluent with SQL. Thus,HIVE makes them feel 
at home while working in a Hadoop Ecosystem.
• Basically, HIVE is a data warehousing component which performs reading, writing and 
managing large data sets in a distributed environment using SQL-like interface.
HIVE + SQL = HQL
• The query language of Hive is called Hive Query Language(HQL), which is very similar 
like SQL.
• It has 2 basic components: Hive Command Line and JDBC/ODBC driver.
• The Hive Command line interface is used to execute HQL commands.
• While, JavaDatabase Connectivity (JDBC) andObject Database Connectivity (ODBC)is 
used to establish connection from data storage.




APACHE MAHOUT

Now, let us talk about Mahout which is renowned for machine learning. Mahout provides an 
environment for creating machine learning applications which are scalable.


APACHE SPARK

• Apache Spark is a framework for real time data analytics in a distributed computing 
environment.
• The Spark is written in Scala and was originally developed at the University of 
California, Berkeley.
• It executes in-memory computations to increase speed of data processing over MapReduce.


APACHE HBASE

• HBase is an open source, non-relational distributed database. In other words, it is a 
NoSQL database.
• It supports all types of data and that is why, it’s capable of handling anything and 
everything inside a Hadoop ecosystem.
• Itis modelled afterGoogle’s BigTable, which is adistributedstorage systemdesigned to 
cope up with large data sets.
• The HBase was designed to run on top of HDFS and provides BigTable like capabilities.
• It gives us a faulttolerant way of storing sparse data, which is common in most Big Data 
use cases.



APACHE DRILL

As the name suggests, Apache Drill is used to drill into any kind of data. It’s an open source 
application which works with distributed environment to analyze large data sets.
• It is a replica of Google Dremel.
• It supports different kinds NoSQL databases and file systems, which is a powerful 
feature ofDrill.
11 |P a g e



APACHE ZOOKEEPER

• ApacheZookeeperis thecoordinator ofanyHadoopjobwhichincludes acombination of 
various services in a Hadoop Ecosystem.
• Apache Zookeeper coordinates with various services in a distributed environment.
APACHE OOZIE
Consider Apache Oozie as a clock and alarm service inside Hadoop Ecosystem. For Apache jobs, 
Oozie has been just like a scheduler. It schedules Hadoop jobs and binds them together as one 
logical work.


APACHE SQOOPand FLUME

Now,let us talk about another data ingesting service i.e. Sqoop. The major difference between 
Flume and Sqoop is that:
• Flume only ingests unstructured data or semi-structured data into HDFS.
• While Sqoop can import as well as export structured data from RDBMS or Enterprise 
data warehouses to HDFS or vice versa.


APACHE SOLR & LUCENE

• Apache Lucene is based on Java, which also helps in spell checking.
• If Apache Lucene is the engine, Apache Solr is the car built around it. Solris a complete 
application built around Lucene.


APACHE AMBARI

Ambari is an Apache Software Foundation Project which aims at making Hadoop ecosystem 
more manageable.








WEEKLY LAB EXCERCISES
Week1: Perform setting up and installing Hadoop
Step1: Visit www.virtualbox.org, look forDownloads to view VirtualBox platform package of 
Windows hosts and click on it to download.
Step2:
a) Double click on VirtualBox-6.1.18-142142-Win to install and the following screen. Click
on Next
b) With the previous clicks, the following appears screen on which click on Next
c) Check all options and click Next
d) Click Install
e) Virtual box is being 
installed
f) VirtualBox has installed and check option to start Oracle VM VirtualBox 6.1.18
g) Oracle VM VirtualBox is being run.
Step 3:
a) Download cloudera-quickstart-vm-5.13.0-0-virtualbox and click File -> ImportAppliance
b) Once Import Appliance is clicked, it prompts to browse for cloudera-quickstart-vm5.13.0-0- virtualbox (Open Virtualization Format) and click Open ->Next
c) It prompts Appliance settings where configuration is made. ClickImport
d) cloudera-quickstart-vm-5.13.0-0 is being imported intoVirtualBox
e) cloudera-quickstart-vm-5.13.0-0 is imported and click Start(Green Coloured
Arrow)to launch Cloudera Environment.