Week-9 : Write a Map-Reduce program that mines weather 
data. Hint: Weather sensors collecting
data every hour at many locations 
across the globe gather a large volume of log data, which is a 
good candidate for analy sis with Map-Reduce, since it is semistructured and record-oriented.
Step‐1: Open Virtual box and then start cloudera 
quickstart Step‐2: Open Eclipse present on the cloudera 
desktop Step‐3: Creating Java Project
3.1 : File‐> New ‐> Project ‐> Java Project ‐> Next 
(“MyMaxMin” is the Project name)
Step‐4: Adding the Hadoop libraries to the Project
4.1 : Click on Add External Jars button, the, File File System > usr > lib> Hadoop, 
select all the libraray (jar) files and then click OK button
4.2 : Now Click on Add External Jars button, the, File File System > usr > lib> 
Hadoop>client, select all the libraray (jar) files and then click OK
button
4.3 : Click finish button
Step‐5: Create Java MapperReduce program
5.1 : In the explorer panel Right click on “src” folder of the project 
MyMaxMin New> Class> Name textfield give as “MyMaxMin” and click
Finish button.
5.2 : Type the code for MyMaxMin program with import files, Mapper class, 
Reducer class Driver class with main method
Step‐6: Export the project as JAR
6.1 : Right click MyMaxMin project and select “Export” >> Java >> Jar file >> 
Next>> in the JAR file textfield give as /home/cloudera/MyMaxMin.jar, click 
Finish button>> OK
6.2 : Open cloudera@quickstart terminal and verify the jar file using ls 
command Step‐7: Create the input file for the MapReduce program by typing
command
cat > /home/cloudera/inputFile.txt
Verify the data contents by cat /home/cloudera/CRND 01 03 
3 -2015-AK _ Fairbanks.txt
Step‐8: Move the input file created in local system to hdfs store by
hdfs dfs ‐put /home/cloudera/inputFile.txt /

view the contents of the file moved to hdfs by typing command
hdfs dfs ‐cat /CRND 01 03 3 -201 5-AK _ Fairbanks.txt
Step‐9: Run MapReduce program on Hadoop by typing command
hadoop jar /home/cloudera/My MaxMin.jar My MaxMin /CRND 01 03 
3 -2015- AK _ Fairbanks.txt
/out25
Each time you run the above command; you need to give different name for 
the output directory.
Step‐10: View the output directory content by hdfs dfs ‐ls /out25 of the program/job 
executed,
hdfs dfs ‐cat /out25/part‐r‐00000


Example: Calculate Maximum and Minimum T emperature

// importing Libraries 
import java.io.IOException; 
import java.util.Iterator;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable; 
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat; 
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat; 
import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat; 
import org.apache.hadoop.mapreduce.lib.input.TextInputFormat; 
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper; 
import org.apache.hadoop.mapreduce.Reducer; 
import org.apache.hadoop.conf.Configuration;
public class MyMaxMin {
// Mapper
/*MaxTemperatureMapper class is static
* and extends Mapper abstract class
* having four Hadoop generics type
* LongWritable, Text, Text, Text.
*/
public static class MaxTemperatureMapper extends
Mapper<LongWritable, Text, Text, Text> {
/**
* @method map
* This method takes the input as a text data type.
* Now leaving the first five tokens, ittakes
* 6th token is taken as temp_max and
* 7th token is taken as temp_min. Now
* temp_max > 30 and temp_min < 15 are
* passed to the reducer.
*/
// the data in our data set with
// this value is inconsistent data 
public static final int MISSING = 9999;
@Override
public void map(LongWritable arg0, Text Value, Context context) 
throws IOException, InterruptedException {
// Convert the single row(Record) to
// String and store it in String
// variable name line
String line = Value.toString();
// Check for the empty line 
if (!(line.length() == 0)) {
// from character 6 to 14 we have
// the date in our dataset
String date = line.substring(6, 14);
//similarly we have taken the maximum
//temperature from 39 to 45 characters
float temp_Max = Float.parseFloat(line.substring(39, 45).trim());
// similarly we have taken the minimum
// temperature from 47 to 53 characters
float temp_Min = Float.parseFloat(line.substring(47, 53).trim());
// if maximum temperature is
// greater than 30, it is a hot day 
if (temp_Max > 30.0) {
// Hot day
context.write(new Text("The Day is Hot Day :" + date),
new
Text(String.valueOf(temp_Max)));
}
// if the minimum temperature is
// less than 15, it is a cold day 
if (temp_Min < 15) {
// Cold day
context.write(new Text("The Day is Cold Day :" + date), 
new Text(String.valueOf(temp_Min)));
}
}
}
}
// Reducer
/*MaxTemperatureReducer class is static 
and extends Reducer abstract class 
having four Hadoop generics type
Text, Text, Text, Text.
*/
public static class MaxTemperatureReducer extends
Reducer<Text, Text, Text, Text> {
/**
* @method reduce
* This method takes the input as key and
* list of values pair from the mapper,
* it does aggregation based on keys and
* produces the final context.
*/
public void reduce(Text Key, Iterator<Text> Values, Context context) 
throws IOException, InterruptedException {
// putting all the values in
// temperature variable of type String
String temperature = Values.next().toString(); 
context.write(Key, new Text(temperature));
}
}
/**
* @method main
* This method is used for setting
* all the configuration properties.
* It acts as a driver for map-reduce
* code.
*/
public static void main(String[] args) throws Exception {
// reads the default configuration of the
// cluster from the configuration XML files 
Configuration conf = new Configuration();
// Initializing the job with the
// default configuration of the cluster
Job job = new Job(conf, "weather example");
// Assigning the driver class name 
job.setJarByClass(MyMaxMin.class);
// Key type coming out of mapper 
job.setMapOutputKeyClass(Text.class);
// value type coming out of mapper 
job.setMapOutputValueClass(Text.class);
// Defining the mapper class name 
job.setMapperClass(MaxTemperatureMapper.class);
// Defining the reducer class name 
job.setReducerClass(MaxTemperatureReducer.class);
// Defining input Format class which is
// responsible to parse the dataset
// into a key value pair 
job.setInputFormatClass(TextInputFormat.class);
// Defining output Format class which is
// responsible to parse the dataset
// into a key value pair 
job.setOutputFormatClass(TextOutputFormat.class);
// setting the second argument
// as a path in a path variable
Path OutputPath = new Path(args[1]);
// Configuring the input path
// from the filesystem into the job 
FileInputFormat.addInputPath(job, new Path(args[0]));
// Configuring the output path from
// the filesystem into the job
FileOutputFormat.setOutputPath(job, new Path(args[1]));
// deleting the context path automatically
// from hdfs so that we don't have
// to delete it explicitly 
OutputPath.getFileSystem(conf).delete(OutputPath);
// exiting the job only if the
// flag value becomes false 
System.exit(job.waitForCompletion(true) ? 0 : 1);
}
}
